[32m[I0107 13:14:51 16209 __main__:<module>:247][0m 
Parameter settings:
log:../logs/set_gen_lm_1
output:../models/set_gen_lm_1
data_path:../data/
split:train
gen_lm:False
nFeats:128
enlarge:0
negative_samples:4
no_use_tree:False
num_workers:1
gru_depth:2
decoder_gru_depth:2
dropout:0
no_bidirectional:False
schedule:[30, 40, 50]
epoch:60
learning_rate:0.0005
samples:6000
batch_size:100
pretrained:
save:1000
lr_decay:0.5
evaluate:none
max_gen_len:200
max_subs_len:75
beam_width:10
interface_pred_model:
interface_gen_model:
interface_payout_model:
timeout:5
passes:10000
hyp_bonus:0
no_use_torch:False
threads:0
num_provers:1
prover_id:0
sample_width:20
random_generate:False
cons_batch:20
expr_path:../exprs/debug
sampling_length:1000
expr_list:['../data/expressions_list']
partial_lm:False
num_props:30000
validall:False
allneg:False
cat:False
data_sampling:1.0
train_neg_only:False
task:pred
num_iters:1000
hardneg:False
num_cons_exprs:3000
proofdir:proofs
num_episode:100
len_episode:40
train_rl:False
debug:False
lm_model:
fake_reward:False
interface_model:
max_len:300
short_file_list:False
repeatable:False
num_old_exprs:1000000.0
new_expr_ratio:0.1
num_gpus:1
sample_dist_ratio:1
gpu_list:[-1]
grad_clip:10
manual_out:False
sampling_time:30
precalculate:False
exprs_pre:
restart:False
notrain:False
stat_model:False
gen_noproof:False
ngram:False
pretraineds:[]
train_from_queue:False
epsilon:1.0
cons_pre_one:False
thm_emb:False
gt_subs:False
gt_prop:False
gt_payout:False
gpu_id:0
graph_classifier:linear
graph_opt:adam
old_vocab:False
n1_jobs:2
n2_jobs:4
prover_settings:[0, 1, 2, 3, 4, 5]
save_score:False
iset:False
cpu:True
bidirectional:True
device:cpu

[31m[W0107 13:14:53 16209 __main__:<module>:295][0m Training Epoch 0 begins.
